<!doctype html>
<html>

<head>
  <title> Delayed Gradient Averaging: Tolerate the Communication Latency for Federated Learning</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
    integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <link href="style_extra.css" rel="stylesheet">
</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <p class="lead" style="font-size:300%;">
        Delayed Gradient Averaging: Tolerate the Communication Latency for Federated Learning
        <address>
                    <nobr>
            <a href="https://lzhu.me">Ligeng Zhu</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="http://www.hongzhoulin.com/">Hongzhou Lin</a>
            <sup>2</sup>
            ,
          </nobr>
                    <nobr>
            <a href="">Yao Lu</a>
            <sup>3</sup>
            ,
          </nobr>
                    <nobr>
            <a href="">Yujun Lin</a>
            <sup>1</sup>
            ,
          </nobr>
                    <nobr>
            <a href="https://songhan.mit.edu">Song Han</a>
            <sup>1</sup>
            
          </nobr>
                    <br>
                    <nobr>Massachusetts Institute of Technology,</nobr>
                    <nobr>Amazon,</nobr>
                    <nobr>Google</nobr>
                  </address>
      </p>
    </div>
  </div> <!-- end nd-pageheader -->

  <div class="container">

    <div class="row">
      <div class="col text-center">
        <p>
                    <a href="assets/neurips19_dga.pdf" class="d-inline-block p-3 align-top">
            <img height="100" width="78" src="assets/paper_preview.jpg" style="border:1px solid"
              data-nothumb><br>
            Paper
          </a>
                    <a href="assets/dga_slides.pdf" class="d-inline-block p-3 align-top">
            <img height="100" width="178" src="assets/dga_slides.jpg" style="border:1px solid"
              data-nothumb><br>
            Slides
          </a>
                </div>
    </div>

    <div class="row">
      <div class="col">

                
                <p><p>Federated Learning is an emerging direction in distributed machine learning that en-ables jointly training a model without sharing the data. Since the data is distributedacross many edge devices through wireless / long-distance connections, federatedlearning suffers from inevitable high communication latency. However, the latencyissues are undermined in the current literature [15] and existing approaches suchas FedAvg [27] become less efficient when the latency increases.   To overcomethe problem, we proposeDelayedGradientAveraging (DGA), which delays theaveraging step to improve efficiency and allows local computation in parallel tocommunication. Wetheoreticallyprove that DGA attains a similar convergencerate as FedAvg, andempiricallyshow that our algorithm can tolerate high networklatency without compromising accuracy. Specifically, we benchmark the trainingspeed on various vision (CIFAR, ImageNet) and language tasks (Shakespeare),with both IID and non-IID partitions, and show DGA can bring 2.55$\times$to 4.07$\times$speedup. Moreover, we built a 16-node Raspberry Pi cluster and show that DGAcan consistently speed up real-world federated learning applications</p></p>
        
                                <h2>Background</h2>
        
                <p><p align="center">
    <img src="assets/dga_background.png" width="80%">
</p>
<p>The training settings of conventional distributed training v.s. federated learning are very different. High latency cost greatly degrades the FedAvg’s performance, proposing a severe challenge to scale up the training system. We need to design algorithms to deal with the latency issue.</p></p>
        
                                <h2>Algorithm</h2>
        
                <p><p align="center">
    <img src="assets/dga_algorithm.png" width="80%">
</p>
<p>The flatten view of our algorithm. The averaging occurs periodically with period K and the delay parameter D naturally shows up indicating the number of gradients between the sending and reception.  The cyan cube in the visualization (a,b) indicates local computation and the yellow cube represents the transmission of the averages. The red bar indicates when the averaging is actually performed.  In DGA, the transmission is in parallel to the computation, which is the main reason why DGA can tolerate high latency.</p></p>
        
                                <h2>Convergence Analysis</h2>
        
                <p><ul>
<li>Assumption 1: The loss function $F(w; x,y )$￼is <strong>Lipchitz smooth</strong>
    $$ \nabla f_{j} (x) - \nabla f_{j} (y) || \le L ||x - y||. \quad \forall x,y \in \mathbb{R}^d $$</li>
<li>Assumption 2: <strong>Bounded gradients and variances</strong>
    $$ E_{\zeta_{j}} || \nabla F_{j} (w; \zeta_{i}) || ^ 2 \le  G^2, \forall w, \forall j,  $$
    $$ E_{\zeta_{j}} || \nabla F_{j} (w; \zeta_{j}) - \nabla f_{j} (w) || ^ 2 \le  \sigma^2, \forall w, \forall j. $$</li>
</ul>
<p>The convergence rate of DGA is $ O (\frac{\Delta+\sigma^2}{\sqrt{JN}}+ \frac{Jd^2}{N}) $ (details in paper).</p>
<p>When $ D &lt; O(N^{\frac{1}{4}} J^{-\frac{3}{4}}) $￼,  DGA converges as fast as original SGD which is $ O (\frac{\Delta+\sigma^2}{\sqrt{JN}}) $￼. </p></p>
        
                                <h2>Accuracy Analysis</h2>
        
                <p><p align="center">
    <img src="assets/dga_acc_table.png" width="60%">
</p>
<p>Comparison of FedAvg and our DGA’s accuracy on 3 datasets with both i.i.d and noni.i.d partitions. The speedup is measured on latency with 1s latency.  Not only DGA demonstrates consistent training speedup, but also DGA maintains the accuracy, on both i.i.d and non-i.i.d partition.</p></p>
        
                                <h2>Speedup Analysis</h2>
        
                <p><p align="center">
    <img src="assets/dga_speedup.png" width="60%">
</p>
<p><em>Left</em>: Our Raspberry Pi farm. Experiments are conducted on two racks. <em>Right</em>: The speedup comparison between FedAvg and DGA~on Raspberry Pi cluster.  On both vision and language tasks, DGA demonstrate consistent improvement over FedAvg.</p></p>
        
                                <h2>Citation</h2>
        
        
                <pre class="highlight"> @inproceedings{zhu2021dga,
    title     = {Delayed Gradient Averaging: Tolerate the Communication Latency in Federated Learning},
    author    = {Zhu, Ligeng and Lin, Hongzhou and Lu, Yao and Lin, Yujun and and Han, Song},
    booktitle = {Annual Conference on Neural Information Processing Systems (NeurIPS)},
    year      = {2021}
} </pre>
                        
                <p><p><strong>Acknowledgments</strong>: We thank MIT-IBM Watson AI Lab, Samsung, Woodside Energy, and NSF CAREER Award #1943349 for supporting this research. Hongzhou Lin acknowledges that the work is done prior to joining Amazon. Yao Lu acknowledges that the work is done prior to joining Google.</p></p>
        
                
      </div>
    </div> <!-- row -->

  </div> <!-- container -->

  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight) {
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight * 50 + "px";
        }
        content.style.height = "550%";
      });
    }
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  
  <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</body>

</html>