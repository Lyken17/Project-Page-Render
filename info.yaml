title: 
    HAT hardware aware transformer

authors:
    Ligeng Zhu: 
        url: https://lzhu.me
        affiliation: 1
    Hnarui Wang: 
        url: http://zhijianliu.com/
        affiliation: 1
    Zhanghao Wu:
        affiliation: 2
    Song Han: 
        url: https://songhan.mit.edu
        affiliation: 1

affiliations:
    - Massachusetts Institute of Technology
    - Shanghai Jiaotong University

beamers:
    NeurIPS 2019 Paper:
        img: assets/9617-deep-leakage-from-gradients.jpg
        url: assets/NeurIPS19_deep_leakage_from_gradients.pdf
        width: 78
        height: 100
    NeurIPS 2019 Poster:
        img: assets/19-nips-dlg.jpg
        url: https://lzhu.me/assets/posters/19-nips-dlg.pdf
        width: 178
        height: 100
    Code:
        img: assets/code.png
        url: https://github.com/mit-han-lab/deep-leakage-from-gradients
        width: 148
        height: 100

paragraphs:
    - rawhtml: >-
        <p align="center">Note: We provide
          <a href="https://colab.research.google.com/gist/Lyken17/91b81526a8245a028d4f85ccc9191884/deep-leakage-from-gradients.ipynb"
            target="_parent"><img
              src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667"
              alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a>
          for quick reproduction!
        </p>
    
    - content: >-
        Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as **Deep Leakage from Gradients** and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is _pixel-wise_ accurate for images and _token-wise_ matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is [gradient pruning.](https://hanlab.mit.edu) 
        $$ \max(2 T_{\text{latency}} + T_{\text{sync}} - d \times T_{\text{computation}}, 0) / T_{\text{compute}} $$
        $$ \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{E} [ \Vert \nabla f(\overline{w}_{n}) \Vert ^2 ]  = O \left (\frac{\Delta+\sigma^2}{\sqrt{JN}}+ \frac{Jd^2}{N} \right ). $$

    - content: >-
        <p align="center">
            <img src="assets/nips-dlg.jpg" width="80%">
        </p>

    - title: Deep Leakage by Gradient Matching
      content: >-
        <p align="center">
            <img src="assets/method.jpg" width="80%">
        </p>
        The core algorithm is to match the gradients between dummy data and real data. It can be implemented in less than 20 lines in PyTorch!
      rawhtml: >-
        <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">deep_leakage_from_gradients</span>(model, origin_grad): 
            dummy_data <span style="color: #333333">=</span> torch<span style="color: #333333">.</span>randn(origin_data<span style="color: #333333">.</span>size())
            dummy_label <span style="color: #333333">=</span>  torch<span style="color: #333333">.</span>randn(dummy_label<span style="color: #333333">.</span>size())
            optimizer <span style="color: #333333">=</span> torch<span style="color: #333333">.</span>optim<span style="color: #333333">.</span>LBFGS([dummy_data, dummy_label] )
          
            <span style="color: #008800; font-weight: bold">for</span> iters <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(<span style="color: #0000DD; font-weight: bold">300</span>):
              <span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">closure</span>():
                optimizer<span style="color: #333333">.</span>zero_grad()
                dummy_pred <span style="color: #333333">=</span> model(dummy_data) 
                dummy_loss <span style="color: #333333">=</span> criterion(dummy_pred, dummy_label) 
                dummy_grad <span style="color: #333333">=</span> grad(dummy_loss, model<span style="color: #333333">.</span>parameters(), create_graph<span style="color: #333333">=</span><span style="color: #007020">True</span>)
          
                grad_diff <span style="color: #333333">=</span> <span style="color: #007020">sum</span>(((dummy_grad <span style="color: #333333">-</span> origin_grad) <span style="color: #333333">**</span> <span style="color: #0000DD; font-weight: bold">2</span>)<span style="color: #333333">.</span>sum() \
                  <span style="color: #008800; font-weight: bold">for</span> dummy_g, origin_g <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">zip</span>(dummy_grad, origin_grad))
                
                grad_diff<span style="color: #333333">.</span>backward()
                <span style="color: #008800; font-weight: bold">return</span> grad_diff
              
              optimizer<span style="color: #333333">.</span>step(closure)
              
            <span style="color: #008800; font-weight: bold">return</span>  dummy_data, dummy_label
          </pre></div>
    
    - title: Results on Image
      content: >-
        <p align="center">
            <img src="assets/demo-crop.gif" width="60%">
        </p>
    
    # - title: Results on Batches Images
    #   content: >-
    #     <p align="center">
    #         <img src="assets/out.gif" width="66%">
    #     </p>
    
    - title: Results on Language Model
      content: >-
        <p align="center">
            <img src="assets/nlp_results.png" width="66%">
        </p>
    
    - title: Introduction Video
      content: >-
        <div class="container_2" align="center">
            <iframe src="https://www.youtube.com/embed/kMVvT8cf3Iw" frameborder="0" allowfullscreen class="video"></iframe>
        </div>

    - title: Citation
      rawhtml: >-
        <pre class="highlight">
        @inproceedings{zhu2019dlg,
            title     = {Deep Leakage from Gradients},
            author    = {Zhu, Ligeng and Liu, Zhijian and and Han, Song},
            booktitle = {Annual Conference on Neural Information Processing Systems (NeurIPS)},
            year      = {2019}
        }
        </pre>
    
    - content: >-
        **Acknowledgments**: We sincerely thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Samsung, Facebook and SONY for supporting this research. We also thank John Cohn for valuable discussion.